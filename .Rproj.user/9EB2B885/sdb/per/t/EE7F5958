{
    "collab_server" : "",
    "contents" : "library(xgboost)\nlibrary(dplyr)\n\nfeats1 = read.csv('data/working/features/features1/feats1.csv')\ntargets1 = read.csv('data/working/features/features1/targets1.csv')\n\n\n# class weights\ncweights = c(1.35298455691,1.38684574053,1.59587388404,1.35318713948,0.347783666015,0.661081706198,1.04723628621,0.398865222651,\n                 0.207586320237,1.50578335208,0.110181365961,1.07803284435,1.36560417316,1.17024113802,1.1933637414,1.1803704493,\n                 1.34414875433,1.11683830693,1.08083910312,0.503152249073)\n\n\n# train DMatrices\ntrains <- lapply(1:20, function(c) xgb.DMatrix(data=data.matrix(feats1), label=as.numeric(targets1[,c]), missing=NA))\n\n\n# parameters\nparams = list(booster = 'gbtree',\n              objective = 'reg:logistic',\n              eta = 0.1,\n              subsample=1,\n              colsample_bytree=1,\n              max_depth=2,\n              verbose=1,\n              silent=1,\n              nthread=3)\n\n\n# cv\ndata <- lapply(1:20, function(c) xgb.DMatrix(data=data.matrix(feats1), label=as.numeric(targets1[,c]), missing=NA))\n\ncustom_cv <- function(nrounds){\n  \n  # folds are of length 1753 1642 1516 1537 1507 1392 1505 1668 1825 1779\n  folds = list(1:1753, 1754:3395, 3396:4911, 4912:6448, 6449:7955, 7956:9347, 9348:10852, 10853:12520, 12521:14345, 14346:16124)\n  \n  \n  booster_folds <- lapply(1:length(folds), function(k){\n    train = lapply(1:20, function(c) xgboost:::slice(data[[c]], folds[[k]]))\n    test = lapply(1:20, function(c) xgboost:::slice(data[[c]], unlist(folds[-k])))\n    booster = lapply(1:20, function(c) xgboost:::xgb.Booster(params, list(train[[c]], test[[c]])))\n    list(train=train, test=test, booster=booster)\n  })\n  \n  for(i in 1:nrounds){\n    msg <- lapply(booster_folds, function(fd){\n      succ = lapply(1:20, function(c) xgboost:::xgb.iter.update(fd$booster[[c]], fd$train[[c]], i - 1, obj=NULL))\n      \n      iter_preds_train = lapply(1:20, function(c) predict(fd$booster[[c]], fd$train[[c]]))\n      BStrain <- function(preds, dtrain){\n        labels = lapply(1:20, function(c) getinfo(fd$train[[c]], 'label'))\n        err = lapply(1:20, function(c) sum(cweights[c]*(iter_preds_train[[c]] - labels[[c]])^2)/length(labels[[c]]))\n        err = sum(unlist(err))\n        return(list(metric='BS', value=err))\n      }\n      msgtrain = lapply(1:20, function(c) xgboost:::xgb.iter.eval(fd$booster[[c]], watchlist = list(train=fd$train[[c]]), i - 1, feval=BStrain))[[1]]\n      \n      iter_preds_test = lapply(1:20, function(c) predict(fd$booster[[c]], fd$test[[c]]))\n      BStest <- function(preds, dtrain){\n        labels = lapply(1:20, function(c) getinfo(fd$test[[c]], 'label'))\n        err = lapply(1:20, function(c) sum(cweights[c]*(iter_preds_test[[c]] - labels[[c]])^2)/length(labels[[c]]))\n        err = sum(unlist(err))\n        return(list(metric='BS', value=err))\n      }\n      booster_copy <- fd$booster\n      msgtest = lapply(1:20, function(c) xgboost:::xgb.iter.eval(booster_copy[[c]], watchlist = list(test=fd$test[[c]]), i - 1, feval=BStest))[[1]]\n      \n      return(c(msgtrain, msgtest))\n    })\n    #print(msg)\n    msg = do.call(rbind, msg) %>% as.data.frame\n    msg = c('train-BS' = paste0(mean(msg[,1]),' + ',sd(msg[,1])), 'test-BS' = paste0(mean(msg[,2]),' + ',sd(msg[,2])))\n    msg\n  }\n  \n}\n\n# train\ncustom_train <- function(nrounds){\n  \n  # boosters\n  boosters = lapply(1:20, function(c){\n    xgboost:::xgb.Booster(params = params, cachelist = list(trains[[c]]))\n  })\n  \n  log = c()\n  \n  for(i in 1:nrounds){\n    \n    # iter update\n    succ = lapply(1:20, function(c) xgboost:::xgb.iter.update(booster=boosters[[c]], dtrain=trains[[c]], iter = i - 1, obj=NULL))\n    \n    # iter predictions\n    iter_preds = lapply(1:20, function(c) predict(boosters[[c]], trains[[c]]))\n\n    # feval = Brier Score\n    BS <- function(preds, dtrain){\n      labels = lapply(1:20, function(c) getinfo(trains[[c]], 'label'))\n      err = lapply(1:20, function(c) sum(cweights[c]*(iter_preds[[c]] - labels[[c]])^2)/length(labels[[c]]))\n      err = sum(unlist(err))\n      return(list(metric='BS', value=err))\n    }\n    \n    # iter eval based on BS\n    msg = lapply(1:20, function(c) xgboost:::xgb.iter.eval(booster=boosters[[c]], watchlist = list(train = trains[[c]]), i - 1, feval=BS))\n    print(unname(msg[[1]]))\n    \n    # log\n    log[i] <- unname(msg[[1]])\n  }\n  return(log)\n}\n\n\nres = custom_train(10)\n\n\n",
    "created" : 1469030012419.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2786117233",
    "id" : "EE7F5958",
    "lastKnownWriteTime" : 1469040986,
    "last_content_update" : 1469040986060,
    "path" : "~/SPHERE-Challenge/code/models/xgboost.R",
    "project_path" : "code/models/xgboost.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}