{
    "collab_server" : "",
    "contents" : "library(xgboost)\nlibrary(dplyr)\n\nfeats1 = read.csv('data/working/features/features1/feats1.csv')\ntargets1 = read.csv('data/working/features/features1/targets1.csv')\n\n\n# class weights\ncweights = c(1.35298455691,1.38684574053,1.59587388404,1.35318713948,0.347783666015,0.661081706198,1.04723628621,0.398865222651,\n                 0.207586320237,1.50578335208,0.110181365961,1.07803284435,1.36560417316,1.17024113802,1.1933637414,1.1803704493,\n                 1.34414875433,1.11683830693,1.08083910312,0.503152249073)\n\n\n# train DMatrices\n#trains <- lapply(1:20, function(c) xgb.DMatrix(data=data.matrix(feats1), label=as.numeric(targets1[,c]), missing=NA))\n\n\n# parameters\nparams = list(booster = 'gbtree',\n              objective = 'reg:logistic',\n              eta = 0.1,\n              subsample=1,\n              colsample_bytree=1,\n              max_depth=2,\n              verbose=1,\n              silent=1,\n              nthread=3)\n\n\n# cv\ndata <- lapply(1:20, function(c) xgb.DMatrix(data=data.matrix(feats1), label=as.numeric(targets1[,c]), missing=NA))\n\ncustom_cv <- function(nrounds){\n  \n  # folds are of length 1753 1642 1516 1537 1507 1392 1505 1668 1825 1779\n  folds = list(1:1753, 1754:3395, 3396:4911, 4912:6448, 6449:7955, 7956:9347, 9348:10852, 10853:12520, 12521:14345, 14346:16124)\n  folds = list(1:1753, 1754:3395, 3396:4911, 4912:6448, 6449:7955, 7956:9347, 9348:10852, 10853:12520, 12521:14345, 14346:16124)\n  \n  \n  booster_folds <- lapply(1:length(folds), function(k){\n    test = lapply(1:20, function(c) xgboost:::slice(data[[c]], folds[[k]]))\n    train = lapply(1:20, function(c) xgboost:::slice(data[[c]], unlist(folds[-k])))\n    booster = lapply(1:20, function(c) xgboost:::xgb.Booster(params, list(train[[c]], test[[c]])))\n    bf = list(train=train, test=test, booster=booster)\n    return(bf)\n  })\n  \n  hist = list()\n  \n  BS <- function(preds, train){\n    labels = lapply(1:20, function(c) getinfo(train[[c]], 'label'))\n    err = lapply(1:20, function(c) sum(cweights[c]*(preds[[c]] - labels[[c]])^2)/length(labels[[c]]))\n    err = sum(unlist(err))\n    return(c('BS'=err))\n  }\n  \n  nrounds=10\n  \n  for(i in 3:30){\n    \n    res <- lapply(booster_folds, function(bf){\n      \n      #succ = lapply(1:20, function(c) xgboost:::xgb.iter.update(bf$booster[[c]], bf$train[[c]], i - 1, obj=NULL))\n      \n      preds_train = lapply(1:20, function(c) predict(bf$booster[[c]], bf$train[[c]]))\n      preds_test = lapply(1:20, function(c) predict(bf$booster[[c]], bf$test[[c]]))\n      \n      \n      preds_train = lapply(1:20, function(c) preds_train[[c]]/colSums(do.call('rbind', preds_train)))\n      preds_test = lapply(1:20, function(c) preds_test[[c]]/colSums(do.call('rbind', preds_test)))\n      \n      eval_train = BS(preds_train, bf$train); names(eval_train) = 'train-BS'\n      eval_test = BS(preds_test, bf$test); names(eval_test) = 'test-BS'\n      \n      rm(preds_train, preds_test, succ)\n      return(c(eval_train, eval_test))\n    })\n    \n    #res = do.call('rbind', res) %>% as.data.frame\n    #res = c(mean(res[,1]), sd(res[,2]), mean(res[,2]), sd(res[,2]))\n    #print(res)\n    #hist[[i]] = res\n    print(i)\n  }\n  \n}\n\n# train\ncustom_train <- function(nrounds){\n  \n  # boosters\n  boosters = lapply(1:20, function(c){\n    xgboost:::xgb.Booster(params = params, cachelist = list(trains[[c]]))\n  })\n  \n  log = c()\n  \n  for(i in 1:nrounds){\n    \n    # iter update\n    succ = lapply(1:20, function(c) xgboost:::xgb.iter.update(booster=boosters[[c]], dtrain=trains[[c]], iter = i - 1, obj=NULL))\n    \n    # iter predictions\n    iter_preds = lapply(1:20, function(c) predict(boosters[[c]], trains[[c]]))\n\n    # feval = Brier Score\n    BS <- function(preds, dtrain){\n      labels = lapply(1:20, function(c) getinfo(trains[[c]], 'label'))\n      err = lapply(1:20, function(c) sum(cweights[c]*(iter_preds[[c]] - labels[[c]])^2)/length(labels[[c]]))\n      err = sum(unlist(err))\n      return(list(metric='BS', value=err))\n    }\n    \n    # iter eval based on BS\n    msg = lapply(1:20, function(c) xgboost:::xgb.iter.eval(booster=boosters[[c]], watchlist = list(train = trains[[c]]), i - 1, feval=BS))\n    print(unname(msg[[1]]))\n    \n    # log\n    log[i] <- unname(msg[[1]])\n  }\n  return(log)\n}\n\n\nres = custom_train(10)\n\n\n",
    "created" : 1469183113259.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "540917874",
    "id" : "AA86E9BD",
    "lastKnownWriteTime" : 1469183505,
    "last_content_update" : 1469183505643,
    "path" : "~/SPHERE-Challenge/code/models/xgboost.R",
    "project_path" : "code/models/xgboost.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}