?xgboost
feats1 = read.csv('data/working/features/features1/feats1.csv')
targets1 = read.csv('data/working/features/features1/targets1.csv')
library(xgboost)
View(targets1)
hist(targets1$a_ascend)
hist(targets1$a_ascend > 0)
hist(targets1$a_ascend > 0 %>% as.numeric)
require(A3)
hist(targets1$a_ascend > 0 %>% as.numeric)
dettach(A3)
detach(A3)
detach(A3)
require(dplyr)
hist(targets1$a_ascend > 0 %>% as.numeric)
hist((targets1$a_ascend > 0) %>% as.numeric)
table((targets1$a_ascend > 0) %>% as.numeric)
table((targets1$a_walk > 0) %>% as.numeric)
library(xgboost)
library(dplyr)
cweights = c(1.35298455691,1.38684574053,1.59587388404,1.35318713948,0.347783666015,0.661081706198,1.04723628621,0.398865222651,
0.207586320237,1.50578335208,0.110181365961,1.07803284435,1.36560417316,1.17024113802,1.1933637414,1.1803704493,
1.34414875433,1.11683830693,1.08083910312,0.503152249073)
params = list(booster = 'gbtree',
objective = 'reg:logistic',
eta = 0.1,
subsample=1,
colsample_bytree=1,
max_depth=2,
verbose=1,
silent=1,
nthread=3)
data <- lapply(1:20, function(c) xgb.DMatrix(data=data.matrix(feats1), label=as.numeric(targets1[,c]), missing=NA))
# folds are of length 1753 1642 1516 1537 1507 1392 1505 1668 1825 1779
folds = list(1:1753, 1754:3395, 3396:4911, 4912:6448, 6449:7955, 7956:9347, 9348:10852, 10853:12520, 12521:14345, 14346:16124)
booster_folds <- lapply(1:length(folds), function(k){
train = lapply(1:20, function(c) xgboost:::slice(data[[c]], folds[[k]]))
test = lapply(1:20, function(c) xgboost:::slice(data[[c]], unlist(folds[-k])))
booster = lapply(1:20, function(c) xgboost:::xgb.Booster(params, list(train[[c]], test[[c]])))
list(train=train, test=test, booster=booster)
})
?xgb.train
xgboost:::xgb.train
xgboost:::categorize.callbacks
?Filter
xgb.cv
xgboost:::xgb.cv
library(xgboost)
library(dplyr)
feats1 = read.csv('data/working/features/features1/feats1.csv')
targets1 = read.csv('data/working/features/features1/targets1.csv')
# class weights
cweights = c(1.35298455691,1.38684574053,1.59587388404,1.35318713948,0.347783666015,0.661081706198,1.04723628621,0.398865222651,
0.207586320237,1.50578335208,0.110181365961,1.07803284435,1.36560417316,1.17024113802,1.1933637414,1.1803704493,
1.34414875433,1.11683830693,1.08083910312,0.503152249073)
# train DMatrices
#trains <- lapply(1:20, function(c) xgb.DMatrix(data=data.matrix(feats1), label=as.numeric(targets1[,c]), missing=NA))
# parameters
params = list(booster = 'gbtree',
objective = 'reg:logistic',
eta = 0.1,
subsample=1,
colsample_bytree=1,
max_depth=2,
verbose=1,
silent=1,
nthread=3)
data <- lapply(1:20, function(c) xgb.DMatrix(data=data.matrix(feats1), label=as.numeric(targets1[,c]), missing=NA))
# folds are of length 1753 1642 1516 1537 1507 1392 1505 1668 1825 1779
folds = list(1:1753, 1754:3395, 3396:4911, 4912:6448, 6449:7955, 7956:9347, 9348:10852, 10853:12520, 12521:14345, 14346:16124)
folds = list(1:1753, 1754:3395, 3396:4911, 4912:6448, 6449:7955, 7956:9347, 9348:10852, 10853:12520, 12521:14345, 14346:16124)
booster_folds <- lapply(1:length(folds), function(k){
test = lapply(1:20, function(c) xgboost:::slice(data[[c]], folds[[k]]))
train = lapply(1:20, function(c) xgboost:::slice(data[[c]], unlist(folds[-k])))
booster = lapply(1:20, function(c) xgboost:::xgb.Booster(params, list(train[[c]], test[[c]])))
bf = list(train=train, test=test, booster=booster)
return(bf)
})
hist = list()
BS <- function(preds, train){
labels = lapply(1:20, function(c) getinfo(train[[c]], 'label'))
err = lapply(1:20, function(c) sum(cweights[c]*(preds[[c]] - labels[[c]])^2)/length(labels[[c]]))
err = sum(unlist(err))
return(c('BS'=err))
}
i=1
res <- lapply(booster_folds, function(bf){
succ = lapply(1:20, function(c) xgboost:::xgb.iter.update(bf$booster[[c]], bf$train[[c]], i - 1, obj=NULL))
#preds_train = lapply(1:20, function(c) predict(bf$booster[[c]], bf$train[[c]]))
#preds_test = lapply(1:20, function(c) predict(bf$booster[[c]], bf$test[[c]]))
#preds_train = lapply(1:20, function(c) preds_train[[c]]/colSums(do.call('rbind', preds_train)))
#preds_test = lapply(1:20, function(c) preds_test[[c]]/colSums(do.call('rbind', preds_test)))
#eval_train = BS(preds_train, bf$train); names(eval_train) = 'train-BS'
#eval_test = BS(preds_test, bf$test); names(eval_test) = 'test-BS'
#rm(preds_train, preds_test, succ)
#return(c(eval_train, eval_test))
})
res
i=2
res <- lapply(booster_folds, function(bf){
succ = lapply(1:20, function(c) xgboost:::xgb.iter.update(bf$booster[[c]], bf$train[[c]], i - 1, obj=NULL))
#preds_train = lapply(1:20, function(c) predict(bf$booster[[c]], bf$train[[c]]))
#preds_test = lapply(1:20, function(c) predict(bf$booster[[c]], bf$test[[c]]))
#preds_train = lapply(1:20, function(c) preds_train[[c]]/colSums(do.call('rbind', preds_train)))
#preds_test = lapply(1:20, function(c) preds_test[[c]]/colSums(do.call('rbind', preds_test)))
#eval_train = BS(preds_train, bf$train); names(eval_train) = 'train-BS'
#eval_test = BS(preds_test, bf$test); names(eval_test) = 'test-BS'
#rm(preds_train, preds_test, succ)
#return(c(eval_train, eval_test))
})
res
for(i in 3:30){
res <- lapply(booster_folds, function(bf){
succ = lapply(1:20, function(c) xgboost:::xgb.iter.update(bf$booster[[c]], bf$train[[c]], i - 1, obj=NULL))
#preds_train = lapply(1:20, function(c) predict(bf$booster[[c]], bf$train[[c]]))
#preds_test = lapply(1:20, function(c) predict(bf$booster[[c]], bf$test[[c]]))
#preds_train = lapply(1:20, function(c) preds_train[[c]]/colSums(do.call('rbind', preds_train)))
#preds_test = lapply(1:20, function(c) preds_test[[c]]/colSums(do.call('rbind', preds_test)))
#eval_train = BS(preds_train, bf$train); names(eval_train) = 'train-BS'
#eval_test = BS(preds_test, bf$test); names(eval_test) = 'test-BS'
#rm(preds_train, preds_test, succ)
#return(c(eval_train, eval_test))
})
#res = do.call('rbind', res) %>% as.data.frame
#res = c(mean(res[,1]), sd(res[,2]), mean(res[,2]), sd(res[,2]))
#print(res)
#hist[[i]] = res
print(i)
}
res <- lapply(booster_folds, function(bf){
#succ = lapply(1:20, function(c) xgboost:::xgb.iter.update(bf$booster[[c]], bf$train[[c]], i - 1, obj=NULL))
preds_train = lapply(1:20, function(c) predict(bf$booster[[c]], bf$train[[c]]))
preds_test = lapply(1:20, function(c) predict(bf$booster[[c]], bf$test[[c]]))
preds_train = lapply(1:20, function(c) preds_train[[c]]/colSums(do.call('rbind', preds_train)))
preds_test = lapply(1:20, function(c) preds_test[[c]]/colSums(do.call('rbind', preds_test)))
eval_train = BS(preds_train, bf$train); names(eval_train) = 'train-BS'
eval_test = BS(preds_test, bf$test); names(eval_test) = 'test-BS'
rm(preds_train, preds_test, succ)
return(c(eval_train, eval_test))
})
res
res = do.call('rbind', res) %>% as.data.frame
res = c(mean(res[,1]), sd(res[,2]), mean(res[,2]), sd(res[,2]))
res
bf =booster_folds[[1]]
preds_train = lapply(1:20, function(c) predict(bf$booster[[c]], bf$train[[c]]))
2.2*10
xgb.cv
lapply(list(list(1), list(2)), function(i) ptin(i))
lapply(list(list(1), list(2)), function(i) print(i))
require(dplyr)
# load
accel <- read.csv('data/raw_data/public_data/train/00001/acceleration.csv')
target <- read.csv('data/raw_data/public_data/train/00001/targets.csv')
pir <- read.csv('data/raw_data/public_data/train/00001/pir.csv')
video_hallway <- read.csv('data/raw_data/public_data/train/00001/video_hallway.csv')
video_kitchen <- read.csv('data/raw_data/public_data/train/00001/video_kitchen.csv')
video_living_room <- read.csv('data/raw_data/public_data/train/00001/video_living_room.csv')
View(accel)
View(target)
which(target$a_ascend > 0)
require(ggplot2)
end = which(target$a_ascend > 0)
e=end[1]
tmp = accel[which(accel$t < e & accel$t >= e-2),]
plot(tmp$x)
plot(x = tmp$t, y = tmp$x)
ggplot(x = tmp$t, y = tmp$x)
for (e in end){
tmp = accel[which(accel$t < e & accel$t >= e-2),]
plot(x = tmp$t, y = tmp$x)
}
end = which(target$a_ascend > 0)
for (e in end){
tmp = accel[which(accel$t < e & accel$t >= e-2),]
plot(x = tmp$t, y = tmp$x, type='l')
}
tmp
end
xgboost:::cb.cv.predict()
xgboost:::cb.cv.predict
predict
xgboost:::predict.xgb.Booster
